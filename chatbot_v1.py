
# pip install jq
# pip install -U langchain langchain-openai langchain-community
# pip install -U langchain langchain-openai


from dotenv import load_dotenv
import os
import openai
from langchain_community.document_loaders import JSONLoader
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
import tiktoken
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_text_splitters import TokenTextSplitter
from langchain_core.retrievers import BaseRetriever
from typing import List
from dataclasses import dataclass
from langchain_core.retrievers import BaseRetriever
from langchain_core.documents import Document
from typing import List
from pydantic import Field
from langchain_core.runnables import RunnableSerializable
from langchain_community.document_loaders import JSONLoader


# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

# 1. JSON ë¡œë” ì„¤ì • - contentsëŠ” page_contentë¡œ, dateëŠ” metadataë¡œ ì´ë™

from dotenv import load_dotenv
import os
import openai
from langchain_community.document_loaders import JSONLoader
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
import tiktoken
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_text_splitters import TokenTextSplitter
from langchain_core.retrievers import BaseRetriever
from typing import List
from dataclasses import dataclass
from langchain_core.retrievers import BaseRetriever
from langchain_core.documents import Document
from typing import List
from pydantic import Field
from langchain_core.runnables import RunnableSerializable
from langchain_community.document_loaders import JSONLoader


# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

# 1. JSON ë¡œë” ì„¤ì • - contentsëŠ” page_contentë¡œ, dateëŠ” metadataë¡œ ì´ë™
loader = JSONLoader(
    file_path="./merged_cbam_data.json",
    jq_schema=".[]",                    # JSON ë°°ì—´ êµ¬ì¡°
    text_content=True,                 # contents â†’ page_content
    content_key="contents",            # page_content í•„ë“œ ì§€ì • (ë§Œì•½ "text"ë©´ ë°”ê¿”ì•¼ í•¨)
    metadata_func=lambda record, _: {
        "date": record.get("date", "unknown"),
        "type": record.get("type", "unknown")  # type í•„ë“œë„ í¬í•¨
    }
)

documents = loader.load()

# 2. Text Splitter ì„¤ì •

splitter = TokenTextSplitter(
    chunk_size=512,
    chunk_overlap=50
)

split_docs = splitter.split_documents(documents)


# 3. ì„ë² ë”© ë° ë²¡í„° ì €ì¥ì†Œ ìƒì„±
embedding_model = OpenAIEmbeddings(
    model="text-embedding-3-large",
    openai_api_key=openai.api_key
)
# - FAISS ë²¡í„° ì €ì¥ì†Œ ìƒì„±
db = FAISS.from_documents(split_docs, embedding_model)


# 5. ë¦¬íŠ¸ë¦¬ë²„ ë° LLM êµ¬ì„±

# - ë‚ ì§œ ì •ë ¬ ë¦¬íŠ¸ë¦¬ë²„ (Pydantic + LangChain ìµœì‹  ë°©ì‹)
class DateSortedRetriever(BaseRetriever, RunnableSerializable):
    base_retriever: BaseRetriever = Field(...)

    def _get_relevant_documents(self, query: str) -> List[Document]:
        docs = self.base_retriever.invoke(query)
        docs_sorted = sorted(
            docs,
            key=lambda d: d.metadata.get("date", "9999-99-99").replace(".", "-")
        )
        return docs_sorted
    
# - ë¦¬íŠ¸ë¦¬ë²„ + ì •ë ¬ ë˜í¼ ì ìš©
retriever = db.as_retriever(search_kwargs={"k": 10})
sorted_retriever = DateSortedRetriever(base_retriever=retriever)



# 6. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜ ë° QA ì²´ì¸ êµ¬ì„±

prompt_template = PromptTemplate.from_template("""
ì•ˆë…•í•˜ì„¸ìš”. ì €ëŠ” 'ì¹´ë´‡'ì´ì—ìš”.  
CBAM(íƒ„ì†Œêµ­ê²½ì¡°ì •ì œë„) ëŒ€ì‘ í”Œë«í¼ì„ ì§ì ‘ ê°œë°œí–ˆê³ , ì§€ê¸ˆì€ ì´ í”Œë«í¼ê³¼ ê´€ë ¨ëœ ë‚´ìš©ì„ ì•ˆë‚´í•´ë“œë¦¬ê³  ìˆì–´ìš”.

ì œê°€ ì°¸ê³ í•˜ëŠ” ë¬¸ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì„œë¡œ ë‹¤ë¥¸ ì—­í• ì„ í•´ìš”:
                            
1. **manual = í”Œë«í¼ ë°˜ì˜ ì‚¬í•­ ì •ë¦¬ ë¬¸ì„œ**  
   ğŸ‘‰ í”Œë«í¼ì´ ì‹¤ì œë¡œ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ êµ¬í˜„ë˜ì—ˆëŠ”ì§€ë¥¼ ì„¤ëª…í•˜ëŠ” ë¬¸ì„œì˜ˆìš”.  
   ğŸ‘‰ 'ì›ë˜ëŠ” ì´ë ‡ê²Œ í•´ì•¼ í•˜ëŠ”ë°, ìš°ë¦¬ í”Œë«í¼ì—ì„œëŠ” ì´ë ‡ê²Œ êµ¬í˜„í–ˆì–´ìš”'ì²˜ëŸ¼ ë¹„êµ ì¤‘ì‹¬ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆì–´ìš”.

2. **guide=  EU íƒ„ì†Œêµ­ê²½ì œë„ ì „í™˜ê¸°ê°„ ì´í–‰ ê°€ì´ë“œë¼ì¸**  
   ğŸ‘‰ ì œë„ì˜ ê¸°ë³¸ì ì¸ ì ˆì°¨ì™€ ë°©ì‹ì´ ì •ë¦¬ëœ ê³µì‹ ê°€ì´ë“œì˜ˆìš”.

3. **cbam= ì•Œê¸°ì‰½ê²Œ í’€ì–´ì“´ CBAM í•´ì„¤ì„œ**  
   ğŸ‘‰ ì œë„ ì „ì²´ì˜ ê°œë…ê³¼ ìš©ì–´ë¥¼ ì‰½ê²Œ ì„¤ëª…í•œ ìë£Œì˜ˆìš”.

4. **omnibus  = EU ì˜´ë‹ˆë²„ìŠ¤ íŒ¨í‚¤ì§€ ë²•ì•ˆ ìš”ì•½**  
   ğŸ‘‰ CBAM ì œë„ê°€ ê°œì •ëœ ë‚´ìš©ì´ ë‹´ê²¨ ìˆê³ , ì¼ë¶€ëŠ” í”Œë«í¼ì—ë„ ë°˜ì˜ë˜ì–´ ìˆì–´ìš”.

---

ğŸ“Œ **ë‹µë³€ ê¸°ì¤€ì€ ì•„ë˜ì™€ ê°™ì•„ìš”:**

- â— ì§ˆë¬¸ì´ ì œë„ ì „ë°˜ì— ëŒ€í•œ ê°œë…ì„ ë¬»ëŠ” ê²½ìš°ì—ëŠ” **ê°€ì´ë“œë¼ì¸ì´ë‚˜ í•´ì„¤ì„œ**ë¥¼ ì°¸ê³ í•´ì„œ ê°œë…ì„ ì„¤ëª…í• ê²Œìš”.
- â— ì§ˆë¬¸ì´ 'ìš°ë¦¬ í”Œë«í¼ì—ì„œ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?'ì²˜ëŸ¼ êµ¬í˜„ ë°©ì‹ì„ ë¬»ëŠ” ê²½ìš°ì—ëŠ” **í”Œë«í¼ ë°˜ì˜ ì‚¬í•­ ì •ë¦¬ ë¬¸ì„œ**ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì„¤ëª…ë“œë¦´ê²Œìš”.
- â— ì„œë¡œ ë‹¤ë¥¸ ë¬¸ì„œ ê°„ ë‚´ìš©ì´ ìƒì´í•  ê²½ìš°, **í”Œë«í¼ êµ¬í˜„ ê¸°ì¤€ì„ ìš°ì„ ** ë”°ë¥´ë˜, í•„ìš”í•˜ë©´ ì°¨ì´ì ë„ í•¨ê»˜ ì•Œë ¤ë“œë¦´ê²Œìš”.
- âŒ ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ë‹µë³€ë“œë¦´ ìˆ˜ ì—†ìœ¼ë©°, ì´ ê²½ìš° ì´ë ‡ê²Œ ì•ˆë‚´í• ê²Œìš”: **"ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."**

ì €ëŠ” ì‹¤ë¬´ìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ, ì§ˆë¬¸ì˜ ë§¥ë½ì— ë”°ë¼ ê°œë…ê³¼ í”Œë«í¼ êµ¬í˜„ì„ êµ¬ë¶„í•´ì„œ ì„¤ëª…í•´ë“œë¦´ ìˆ˜ ìˆì–´ìš”.  
ê¶ê¸ˆí•œ ì ì´ ìˆë‹¤ë©´ ì–¸ì œë“ ì§€ í¸í•˜ê²Œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”!


ë¬¸ì„œ:
{context}

ì§ˆë¬¸:
{question}
""")

llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0.5)
# gpt-3.5-turbo
qa_chain = RetrievalQA.from_chain_type (
    llm=llm,
    retriever=sorted_retriever,
    chain_type="stuff",
    chain_type_kwargs={"prompt": prompt_template},
    return_source_documents=True  
)

# 7. íˆìŠ¤í† ë¦¬ ê¸°ë°˜ ì±—ë´‡ í´ë˜ìŠ¤ ì •ì˜
class CBAMChatbot:
    def __init__(self, qa_chain):
        self.qa_chain = qa_chain
        self.history = []

    def ask(self, user_query: str):

        # í”„ë¡¬í”„íŠ¸ íˆìŠ¤í† ë¦¬ êµ¬ì„±
        history_prompt = ""
        for u, b in self.history[-5:]:
            history_prompt += f"User: {u}\nBot: {b}\n"

        # ì§ˆì˜ ìˆ˜í–‰
        result = self.qa_chain.invoke({"query": user_query})
        answer = result["result"]
        sources = result["source_documents"]

        # íˆìŠ¤í† ë¦¬ ì €ì¥
        self.history.append((user_query, answer))

        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "=" * 60)
        print("â“ ì§ˆë¬¸:")
        print(user_query)
        print("\n" + "-" * 60)
        print("ğŸ¤– ë‹µë³€:\n")
        print(answer)
        print("\n" + "-" * 60)
        print("ğŸ“š ì°¸ê³  ë¬¸ì„œ ì¶œì²˜ (ë‚ ì§œ ê¸°ì¤€):\n")

        for i, doc in enumerate(sources, start=1):
            date = doc.metadata.get("date", "N/A")
            type = doc.metadata.get("type", "N/A")
            preview = doc.page_content.strip().replace("\n", " ")[:400] + "..."  # ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
            print(f"{i}. ë‚ ì§œ: {date} / ë‚´ìš©: {preview} / ìœ í˜•: {type}")

        print("=" * 60 + "\n")
        return answer

# 8. ì±—ë´‡ ê°ì²´ ìƒì„± ë° í…ŒìŠ¤íŠ¸
chatbot = CBAMChatbot(qa_chain)

while True:
    user_input = input("\nâ“ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œí•˜ë ¤ë©´ 'exit'): ")
    if user_input.lower() in ['exit', 'quit']:
        break
    chatbot.ask(user_input)
