
# pip install jq
# pip install -U langchain langchain-openai langchain-community
# pip install -U langchain langchain-openai


from dotenv import load_dotenv
import os
import openai
from langchain_community.document_loaders import JSONLoader
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
import tiktoken
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_text_splitters import TokenTextSplitter
from langchain_core.retrievers import BaseRetriever
from typing import List
from dataclasses import dataclass
from langchain_core.retrievers import BaseRetriever
from langchain_core.documents import Document
from typing import List
from pydantic import Field
from langchain_core.runnables import RunnableSerializable


# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

# 1. JSON ë¡œë” ì„¤ì • - contentsëŠ” page_contentë¡œ, dateëŠ” metadataë¡œ ì´ë™
loader = JSONLoader(
    file_path="./merged_cbam_data.json",
    jq_schema=".[]",                   # JSON ë°°ì—´ êµ¬ì¡°
    text_content=True,                # contents â†’ page_content
    content_key="contents",           # page_content í•„ë“œ ì§€ì •
    metadata_func=lambda record, _: {"date": record.get("date", "unknown")} 
)

documents = loader.load()

# í™•ì¸
print("ë¬¸ì„œ ê°œìˆ˜:", len(documents))
print("ìƒ˜í”Œ ë¬¸ì„œ:", documents[0])

documents[:5]

import tiktoken
# ëª¨ë¸ì— ë§ëŠ” í† í¬ë‚˜ì´ì € ì„ íƒ
encoding = tiktoken.encoding_for_model("text-embedding-3-large")

# ì˜ˆì‹œ: ì²« ë²ˆì§¸ ë¬¸ì„œì˜ í…ìŠ¤íŠ¸
sample_text = documents[0].page_content

# í† í° ê°œìˆ˜ ê³„ì‚°
tokens = encoding.encode(sample_text)
print("í† í° ê°œìˆ˜:", len(tokens))


# ì „ì²´ ë¬¸ì„œ í‰ê·  í† í° ìˆ˜ ê³„ì‚°
token_counts = [len(encoding.encode(doc.page_content)) for doc in documents]
avg_tokens = sum(token_counts) / len(token_counts)
max_tokens = max(token_counts)

print(f"ë¬¸ì„œ ê°œìˆ˜: {len(token_counts)}")
print(f"í‰ê·  í† í° ìˆ˜: {avg_tokens:.2f}")
print(f"ìµœëŒ€ í† í° ìˆ˜: {max_tokens}")
# -----------------------------------------# 


# 2. Text Splitter ì„¤ì •

splitter = TokenTextSplitter(
    chunk_size=512,
    chunk_overlap=50
)

split_docs = splitter.split_documents(documents)


print(f"ğŸ”¹ ì´ ì²­í¬ ê°œìˆ˜: {len(split_docs)}")  # ì˜ˆ: 3500ê°œ ë“±
print("ğŸ”¹ ì˜ˆì‹œ ì²­í¬:", split_docs[0])


# 4. ì„ë² ë”© ë° ë²¡í„° ì €ì¥ì†Œ ìƒì„±
embedding_model = OpenAIEmbeddings(
    model="text-embedding-3-large",
    openai_api_key=openai.api_key
)

# 5. FAISS ë²¡í„° ì €ì¥ì†Œ ìƒì„±
db = FAISS.from_documents(split_docs, embedding_model)
# 6. ë¦¬íŠ¸ë¦¬ë²„ ë° LLM êµ¬ì„±

# âœ… ë‚ ì§œ ì •ë ¬ ë¦¬íŠ¸ë¦¬ë²„ (Pydantic + LangChain ìµœì‹  ë°©ì‹)
class DateSortedRetriever(BaseRetriever, RunnableSerializable):
    base_retriever: BaseRetriever = Field(...)

    def _get_relevant_documents(self, query: str) -> List[Document]:
        docs = self.base_retriever.get_relevant_documents(query)
        docs_sorted = sorted(
            docs,
            key=lambda d: d.metadata.get("date", "9999-99-99").replace(".", "-")
        )
        return docs_sorted
# âœ… ë¦¬íŠ¸ë¦¬ë²„ + ì •ë ¬ ë˜í¼ ì ìš©
retriever = db.as_retriever(search_kwargs={"k": 10})
sorted_retriever = DateSortedRetriever(base_retriever=retriever)



# 7. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜ ë° QA ì²´ì¸ êµ¬ì„±

prompt_template = PromptTemplate.from_template("""
ì•ˆë…•í•˜ì„¸ìš”. ì €ëŠ” 'ì¹´ë´‡'ì´ì—ìš”.  
CBAM(íƒ„ì†Œêµ­ê²½ì¡°ì •ì œë„) ëŒ€ì‘ í”Œë«í¼ì„ ì§ì ‘ ê°œë°œí–ˆê³ , ì§€ê¸ˆì€ ì´ í”Œë«í¼ê³¼ ê´€ë ¨ëœ ë‚´ìš©ì„ ì•ˆë‚´í•´ë“œë¦¬ê³  ìˆì–´ìš”.

ì œê°€ ì°¸ê³ í•˜ëŠ” ë¬¸ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì„œë¡œ ë‹¤ë¥¸ ì—­í• ì„ í•´ìš”:

1. **í”Œë«í¼ ë°˜ì˜ ì‚¬í•­ ì •ë¦¬ ë¬¸ì„œ**  
   ğŸ‘‰ í”Œë«í¼ì´ ì‹¤ì œë¡œ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ êµ¬í˜„ë˜ì—ˆëŠ”ì§€ë¥¼ ì„¤ëª…í•˜ëŠ” ë¬¸ì„œì˜ˆìš”.  
   ğŸ‘‰ 'ì›ë˜ëŠ” ì´ë ‡ê²Œ í•´ì•¼ í•˜ëŠ”ë°, ìš°ë¦¬ í”Œë«í¼ì—ì„œëŠ” ì´ë ‡ê²Œ êµ¬í˜„í–ˆì–´ìš”'ì²˜ëŸ¼ ë¹„êµ ì¤‘ì‹¬ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆì–´ìš”.

2. **EU íƒ„ì†Œêµ­ê²½ì œë„ ì „í™˜ê¸°ê°„ ì´í–‰ ê°€ì´ë“œë¼ì¸**  
   ğŸ‘‰ ì œë„ì˜ ê¸°ë³¸ì ì¸ ì ˆì°¨ì™€ ë°©ì‹ì´ ì •ë¦¬ëœ ê³µì‹ ê°€ì´ë“œì˜ˆìš”.

3. **ì•Œê¸°ì‰½ê²Œ í’€ì–´ì“´ CBAM í•´ì„¤ì„œ**  
   ğŸ‘‰ ì œë„ ì „ì²´ì˜ ê°œë…ê³¼ ìš©ì–´ë¥¼ ì‰½ê²Œ ì„¤ëª…í•œ ìë£Œì˜ˆìš”.

4. **EU ì˜´ë‹ˆë²„ìŠ¤ íŒ¨í‚¤ì§€ ë²•ì•ˆ ìš”ì•½**  
   ğŸ‘‰ CBAM ì œë„ê°€ ê°œì •ëœ ë‚´ìš©ì´ ë‹´ê²¨ ìˆê³ , ì¼ë¶€ëŠ” í”Œë«í¼ì—ë„ ë°˜ì˜ë˜ì–´ ìˆì–´ìš”.

---

ğŸ“Œ **ë‹µë³€ ê¸°ì¤€ì€ ì•„ë˜ì™€ ê°™ì•„ìš”:**

- â— ì§ˆë¬¸ì´ ì œë„ ì „ë°˜ì— ëŒ€í•œ ê°œë…ì„ ë¬»ëŠ” ê²½ìš°ì—ëŠ” **ê°€ì´ë“œë¼ì¸ì´ë‚˜ í•´ì„¤ì„œ**ë¥¼ ì°¸ê³ í•´ì„œ ê°œë…ì„ ì„¤ëª…í• ê²Œìš”.
- â— ì§ˆë¬¸ì´ 'ìš°ë¦¬ í”Œë«í¼ì—ì„œ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?'ì²˜ëŸ¼ êµ¬í˜„ ë°©ì‹ì„ ë¬»ëŠ” ê²½ìš°ì—ëŠ” **í”Œë«í¼ ë°˜ì˜ ì‚¬í•­ ì •ë¦¬ ë¬¸ì„œ**ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì„¤ëª…ë“œë¦´ê²Œìš”.
- â— ì„œë¡œ ë‹¤ë¥¸ ë¬¸ì„œ ê°„ ë‚´ìš©ì´ ìƒì´í•  ê²½ìš°, **í”Œë«í¼ êµ¬í˜„ ê¸°ì¤€ì„ ìš°ì„ ** ë”°ë¥´ë˜, í•„ìš”í•˜ë©´ ì°¨ì´ì ë„ í•¨ê»˜ ì•Œë ¤ë“œë¦´ê²Œìš”.
- âŒ ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ë‹µë³€ë“œë¦´ ìˆ˜ ì—†ìœ¼ë©°, ì´ ê²½ìš° ì´ë ‡ê²Œ ì•ˆë‚´í• ê²Œìš”: **"ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."**

ì €ëŠ” ì‹¤ë¬´ìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ, ì§ˆë¬¸ì˜ ë§¥ë½ì— ë”°ë¼ ê°œë…ê³¼ í”Œë«í¼ êµ¬í˜„ì„ êµ¬ë¶„í•´ì„œ ì„¤ëª…í•´ë“œë¦´ ìˆ˜ ìˆì–´ìš”.  
ê¶ê¸ˆí•œ ì ì´ ìˆë‹¤ë©´ ì–¸ì œë“ ì§€ í¸í•˜ê²Œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”!


ë¬¸ì„œ:
{context}

ì§ˆë¬¸:
{question}
""")

llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0.3, max_tokens=2000)
qa_chain = RetrievalQA.from_chain_type (
    llm=llm,
    retriever=sorted_retriever,
    chain_type="stuff",
    chain_type_kwargs={"prompt": prompt_template},
    return_source_documents=True  
)

# 8. íˆìŠ¤í† ë¦¬ ê¸°ë°˜ ì±—ë´‡ í´ë˜ìŠ¤ ì •ì˜
class CBAMChatbot:
    def __init__(self, qa_chain):
        self.qa_chain = qa_chain
        self.history = []

    def ask(self, user_query: str):
        # í”„ë¡¬í”„íŠ¸ íˆìŠ¤í† ë¦¬ êµ¬ì„±
        history_prompt = ""
        for u, b in self.history[-5:]:
            history_prompt += f"User: {u}\nBot: {b}\n"

        # ì§ˆì˜ ìˆ˜í–‰
        result = self.qa_chain({"query": user_query})
        answer = result["result"]
        sources = result["source_documents"]

        # íˆìŠ¤í† ë¦¬ ì €ì¥
        self.history.append((user_query, answer))

        # ê²°ê³¼ ì¶œë ¥
        print("ğŸ’¬ ì§ˆë¬¸:", user_query)
        print("ğŸ¤– ë‹µë³€:", answer)
        print("\nğŸ“š ì°¸ê³  ë¬¸ì„œ ì¶œì²˜:")
        for doc in sources:
            print(f"- ë‚ ì§œ: {doc.metadata.get('date', 'N/A')} / ë‚´ìš© ì¼ë¶€: {doc.page_content[:60]}...")

        return answer

# 9. ì±—ë´‡ ê°ì²´ ìƒì„± ë° í…ŒìŠ¤íŠ¸
chatbot = CBAMChatbot(qa_chain)

while True:
    user_input = input("\nâ“ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œí•˜ë ¤ë©´ 'exit'): ")
    if user_input.lower() in ['exit', 'quit']:
        break
    chatbot.ask(user_input)